---
title: "Proceso de reclamaciones en una aseguradora: Estimaciones y simulaciones de la Severidad total"
author: "Bernardo Mondragón Brozon, Karen Delgado Curiel, Diego Gonzalez Garcia-Santoyo"
date: "Diciembre 10, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

<!-- Configure R Markdown file -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = TRUE)

# Create beautiful graphs
#install.packages("ggplot")
library(ggplot2)

# TeX expressions in graphs labels
#install.packages("latex2exp")
library(latex2exp)

# Calculate Kurtosis skew and other functions
#install.packages("moments")
library(moments)

# Para knitear
#install.packages("knitr")
library(knitr)

# Para imprimir tablas
#install.packages("kableExtra")
#library(kableExtra)

# Para calcular estimadores bootstrap
#install.packages("bootstrap")
library(bootstrap)

# Color?
colorImp <- TRUE
# Colores chidos
color.primary <- "#be58a0"
color.secondary <- "#00e6e6"
set.seed(7031996)
```

\section{El modelo colectivo}

El negocio de seguros está sujeto a dos tipos esencialmente diferentes de riesgo, riesgos comerciales y riesgos de seguro. Hay dos puntos de vista desde los cuales se puede considerar la teoría del riesgo, la colectiva y la individual o clásica.

En la teoría del riesgo colectivo, se busca investigar directamente la empresa en su conjunto. El interés primario se centra no en las ganancias, pérdidas o reclamaciones de pólizas individuales, sino en el monto de las reclamaciones totales o la ganancia total que surja de todas las pólizas en la cartera considerada.

La teoría del riesgo colectivo considera dos problemas principales: encontrar las funciones de distribución de la ganancia total o el monto total de las reclamaciones en una cartera o empresa de riesgo, y encontrar la probabilidad de que la reserva de riesgo de una empresa se agote.

En este trabajo se ilustran los métodos que sigue una compañía aseguradora para obtener la distribución de la severidad total a la que está expuesta a lo largo de un año. Primero, se supondrá una distribución para la frecuencia con la que llegan las reclamaciones y una distribución para cada monto reclamado. De esta manera se pondrán obtener las probabilidades teóricas con las que la compañía estará egresando cantidades anualmente. Después, mediante técnicas de muestreo, se realizarán simulaciones de estas cantidades para obtener distribuciones empíricas a las cuales se les realizarán pruebas de bondad de ajuste para comprobar que efectivamente los datos observados provienen de la distribución propuesta en un principio.

En la práctica, los actuarios no proponen una distribución para la frecuencia y la severidad, sino que ocupan la información de las bases de datos y realizan pruebas de bondad de ajuste para determinar la distribución de los datos capturados. Una vez determinada las distribuciones que mejor se ajustan a los datos, se realizan las simulaciones bajo diferentes escenarios para analizar el impacto que tienen estos eventos en la situación financiera de la compañía. 

Cuando no se cuentan con bases de datos, entonces se procede suponiendo distribuciones y ajustándolas bayesianamente conforme se observan más datos. Posteriormente, para calcular probabilidades sobre estas distribuciones se realizan las simulaciones. En este trabajo no se realizará ningún análisis bayesiano sobre las distribuciones, pues se trabaja únicamente con las distribuciones propuestas.

\subsection{La severidad total}

En el proceso de reclamaciones de una aseguradora, la severidad total, que es la cantidad total de dinero que la aseguradora terminará pagando en el periodo de estudio, esta dada por la siguiente cantidad:

$$ S=x_1+x_2+\cdots+x_N=\sum_{i=1}^{N}x_i$$

```{r, parametros del modelo, include=FALSE}
lambda_po <- 0.1
siniestro.promedio <- 10000 # Por la parametrizacion de la exponencial
lambda_exp <- 1/siniestro.promedio
```

En donde los riesgos $\{x_i\}_{i=1\dots N}$ son variables aleatorias independientes e idénticamente distribuidas que indican los montos de las reclamaciones. Se puede suponer que $x_i$ sigue una distribución exponencial con media $1/\lambda_{exp}=`r siniestro.promedio`$ para toda $i$. Como la llegada de una reclamación a la aseguradora por parte de un asegurado es un suceso "extraño", podemos suponer que el proceso de llegada de reclamaciones a la aseguradora es un proceso de Poisson, en donde el número $N$ de reclamaciones que llegan a la aseguradora dentro de un año es variable aleatoria que tiene una distribución Poisson con media $\lambda_{Po}=`r lambda_po`$ independiente de las $x_i$'s, de manera que a lo largo del año se observará en promedio $`r lambda_po`$ reclamaciones.

Por el teorema de probabilidad total, se tiene que 

$$Pr\{S\leq s|N=n\}=\frac{Pr\{S\leq s,N=n\}}{Pr\{N=n\}}  $$

$$\Rightarrow \quad Pr\{S\leq s,N=n\}=Pr\{N=n\}Pr\{S\leq s|N=n\}.$$

Por lo tanto se tiene que 
$$\begin{aligned}
  F_S(s) &= \sum_{n=0}^{\infty}Pr\{S\leq s,N=n\}  \\
            &= \sum_{n=0}^{\infty}Pr\{N=n\}Pr\{S\leq s|N=n\} \\
            &= \sum_{n=0}^{\infty}Pr\{N=n\}Pr\{\sum_{i=n}^{N}x_i\leq s|N=n\} \\
            &= \sum_{n=0}^{\infty}Pr\{N=n\}F_X^{*n}(s).
\end{aligned}$$

En donde $F_X^{*n}(s)$ es la convolución de las variables aleatorias $\{x_i\}_{i=1\dots N}$, o bien, la función de distribución de probabilidad acumulada de $\sum_{i=n}^{N}x_i|N=n$ con $x_i \sim exp(\lambda_{exp}=1/`r  siniestro.promedio`)$. 
Sean $M_{\sum_{i=n}^{N}x_i|N=n}(t)$ y $M_{x_1}(t)$ las funciones generadoras de momentos de las variables aleatorias $\sum_{i=n}^{N}x_i|N=n$ y $x_1$ respectivamente, entonces, sin pérdida de generalidad, se tiene que

$$M_{\sum_{i=n}^{N}x_i|N=n}(t)=\prod_{i=1}^n M_{x_1}(t)=\prod_{i=1}^n\frac{\lambda_{exp}}{\lambda_{exp}+t}=\left(\frac{\lambda_{exp}}{\lambda_{exp}+t}\right)^n=\left(1-\frac{t}{\lambda_{exp}}\right)^{-n}.$$

Lo cual corresponde a la funcion generadora de momentos de una variable aleatoria distribuida Gamma con parámetros de $\alpha = n$ y $\beta = \lambda_{exp}$, de manera que 
$$F_X^{*n}(s)=\int_0^s \frac{\lambda_{exp}^n}{\Gamma (n)}t^{n-1}e^{-\lambda_{exp}t} \, \mathrm{d}t.$$

Ademas, la distribución de la frecuencia $N$ (el número de siniestros ocurridos) es Poisson con parámetros $\lambda_{Po}=`r lambda_po`$, entonces se sigue que

$$F_S(s)=\sum_{n=0}^{\infty}\frac{\lambda_{Po}^n e^{-\lambda_{Po}}}{n!}\int_0^s \frac{\lambda_{exp}^n}{\Gamma (n)}t^{n-1}e^{-\lambda_{exp}t} \, \mathrm{d}t. $$

```{r plot S CDF, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
# CDF Compound Poisson Distribution 
cdf.comp.pois <- function(x, lambda_po=4,lambda_exp=2) {
  # Dont take into account terms of the sum that are smaller than the following tolerance
  tolerance <- 0.000001
  # Define the region of integration
  B <- c(0, x)
  # Calculate the sum
  # prob starts in zero because the severity is a positive random variable
  # First term
  prob <- 0 
  if (B[1] == 0) prob <- exp(-lambda_po)
  # The rest of terms
  n <- 1
  repeat {
    next_term <- exp(-lambda_po+n*log(lambda_po)-lfactorial(n)) * pgamma(B[2], shape=n, rate=lambda_exp)
    if (next_term < tolerance) {
      break
    }
    prob <- prob + next_term
    n <- n + 1
  }
  prob
}

# Get values to be ploted
getValues.cdf.comp.pois <- function(x, lambda_po=4, lambda_exp=2) {
  sapply(x, FUN=cdf.comp.pois, lambda_po=lambda_po,lambda_exp=lambda_exp)
}

# Create a plot
p <- ggplot()

# Plot axes to make it look nicer
xAxis <- geom_hline(yintercept = 0, size = .5)
yAxis <-geom_vline(xintercept = 0, size = .5)

# Plot axes labels
cdf.comp.pois.labels <- labs(x = TeX('$s$'), y = TeX('$F_S(s)$'))
# Title
cdf.comp.pois.title <- ggtitle("Función de probabilidad acumulada \n de la severidad total")
# Center title
title.theme <- theme(plot.title = element_text(hjust = 0.5))

# Insert a horizontal line on y=1 because we are ploting a CDF
asinth.axis <- geom_hline(yintercept=1, size=.1)

# Color
if (colorImp) cdf.comp.pois.color=color.primary else cdf.comp.pois.color="black"

# Create layer
cdf.comp.pois.layer <- stat_function(
  aes(x=c(0,10)),
  fun=getValues.cdf.comp.pois,
  args=list(lambda_po=lambda_po, lambda_exp=lambda_exp),
  color=cdf.comp.pois.color
)

# Adjust scales on axes
cdf.xlim <- xlim(0,80000)
cdf.ylim <- ylim(0.88,1)

# Plot the standard CDF
p + xAxis + yAxis + cdf.comp.pois.labels + cdf.comp.pois.title + title.theme + asinth.axis + cdf.comp.pois.layer + cdf.xlim + cdf.ylim
```

Derivando la función de distribución de probabilidad acumulada anterior, se obtiene la función de densidad de probabilidad de la severidad total $S$:

$$\begin{aligned}
  f_S(s) &= \frac{d}{ds}F_S(s)  \\
            &= \frac{d}{ds}\sum_{n=0}^{\infty}\frac{\lambda_{Po}^n e^{-\lambda_{Po}}}{n!}\int_0^s \frac{\lambda_{exp}^n}{\Gamma (n)}t^{n-1}e^{-\lambda_{exp}t} \, \mathrm{d}t \\
            &= \sum_{n=0}^{\infty}\frac{\lambda_{Po}^n e^{-\lambda_{Po}}}{n!}\frac{d}{ds}\int_0^s \frac{\lambda_{exp}^n}{\Gamma (n)}t^{n-1}e^{-\lambda_{exp}t} \, \mathrm{d}t \\
            &= \sum_{n=0}^{\infty}\frac{\lambda_{Po}^n e^{-\lambda_{Po}}}{n!} \frac{\lambda_{exp}^n}{\Gamma (n)}s^{n-1}e^{-\lambda_{exp}s}.
\end{aligned}$$

```{r plot S PDF, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
# PDF Compound Poisson Distribution 
pdf.comp.pois <- function(x, lambda_po=4,lambda_exp=2) {
  # Dont take into account terms of the sum that are smaller than the following tolerance 
  tolerance <- 0.0000000001
  prob <- 0
  # The rest of terms
  n <- 1
  repeat {
    next_term <- exp(-lambda_po+n*log(lambda_po)-lfactorial(n)) * dgamma(x, shape=n, rate=lambda_exp)
    if (next_term < tolerance) {
      break
    }
    prob <- prob + next_term
    n <- n + 1
  }
  prob
}

# Get values to be ploted
getValues.pdf.comp.pois <- function(x, lambda_po=4, lambda_exp=2) sapply(x, FUN=pdf.comp.pois, lambda_po=lambda_po,lambda_exp=lambda_exp)

# Plot axes labels
pdf.comp.pois.labels <- labs(x = TeX('$s$'), y = TeX('$f_S(s)$'))

# Title
pdf.comp.pois.title <- ggtitle("Función de densidad de probabilidad \n de la severidad total")

# Color
if (colorImp) pdf.comp.pois.color <- color.secondary else pdf.comp.pois.color <- "black"

# Create layer
pdf.comp.pois.layer <- stat_function(
  aes(x=c(0,10)),
  fun=getValues.pdf.comp.pois,
  args=list(lambda_po=lambda_po, lambda_exp=lambda_exp),
  color=pdf.comp.pois.color
)

# Adjust scales on axes
pdf.xlim <- xlim(0,80000)
pdf.ylim <- ylim(0,.00001)

# Plot the standard PDF
p + xAxis + yAxis + pdf.comp.pois.labels + pdf.comp.pois.title + title.theme + pdf.comp.pois.layer + pdf.xlim + pdf.ylim
```

A continuación se presenta una tabla que indica la probabilidad que se acumula en ciertos puntos de la distribución de la severidad total:
```{r percentile table, echo=FALSE, warning=FALSE}
# Percentiles vector
percentiles <- c(0,1,50,100,1000,10000,20000,40000,50000)
probs <- sapply(percentiles, FUN=cdf.comp.pois, lambda_po=lambda_po, lambda_exp=lambda_exp)
percentiles.df <- data.frame(percentiles, probs)
names(percentiles.df) <- c("s","F(s)")
kableExtra::kable(percentiles.df)
```

Como se puede observar, casi toda la probabilidad se acumula en valores muy pequeños para la severidad total. Esto se debe a que el número promedio de siniestros ocurridos $\lambda_{Po}=`r lambda_po`$ es muy pequeño y la distribución de cada uno de los montos a indeminizar de los siniestros ocuridos siguie una distribución exponencial. 

\subsection{Simulación de frecuencia y severidad}

Para hacer simulaciones de la severidad total, primero hay que simular el número $N$ de siniestros ocurridos, el cual proviene de la distribución de Poisson con media $\lambda_{Po}=`r lambda_po`$. Una vez simulado el número $N$ de siniestros, se simulan $N$ siniestros, que son variables aleatorias exponenciales con parámetros $\lambda_{exp}=1/`r siniestro.promedio`$. Después de obtener los $N$ siniestros, se suman y de esta manera se obtiene un primer valor de la severidad total. Para concluir con la simulación, se repite el proceso anterior $10000$ veces. 

A continuación se muestran los primeros $100$ valores obtenidos de la simulacion:

```{r simulated values, echo=FALSE, warning=FALSE, message=FALSE}
severities <- c()
n <- 10000
for(i in 1:n) {
  N <- rpois(1,lambda=lambda_po)
  if (N != 0) {
    exps <- rexp(N, rate=lambda_exp)
    severities <- c(severities, sum(exps))
  } else {
    severities <- c(severities, 0)
  }
}
# Imprimiendo solo 100
kable(matrix(severities[1:100],9))
```

El histograma de los valores obtenidos por simulación debe ajustarse a la función de densidad de probabilidad (la linea punteada indica la severidad promedio):

```{r histogram, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
if (colorImp) {
  hist.color <- color.primary
  hist.fill <- color.primary
  hist.alpha <- "0.2"
} else {
  hist.color <- "black"
  hist.fill <- "white"
  hist.alpha <- "1"
}

severities.df <- data.frame(severities)
breaks <- seq(1, 80000, by=1000)
hist.p <- ggplot(data=severities.df, aes(x=severities)) +
  geom_histogram(breaks=breaks, color=hist.color, fill=hist.fill, alpha=hist.alpha) + 
  labs(x = TeX('Severidad total$'), y = TeX('Frecuencia')) +
  ggtitle("Histograma de la severidad total S") + title.theme + geom_vline(xintercept = mean(severities), linetype="dashed", 
                color = color.secondary, size=1)
hist.p
```

En la siguiente gráfica se puede apreciar que, en efecto, el histograma se parece a la función de densidad de probabilidad (nótese el cambio de escala en el eje y):

```{r histogram with density, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
# Plot the PDF with scaled histogram
# Create a plot
hist.den.p <- ggplot()
hist.p2 <- hist.den.p + xAxis + yAxis + pdf.comp.pois.labels + pdf.comp.pois.title + title.theme + pdf.xlim + pdf.ylim + geom_histogram(data=severities.df, aes(x=severities, y=.0000001*..count..), breaks=breaks, color=hist.color, fill=hist.fill, alpha=hist.alpha) + pdf.comp.pois.layer
hist.p2
# For more info about ploting density and histogram
# https://stat.ethz.ch/pipermail/r-help/2011-June/280588.html
```

\subsection{Distribución empírica}

La distribución empírica de la severidad total está dada por

```{r, Preg. 2.3 Distribucion empirica de la severidad, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
df <- data.frame(severities)
theme <- theme(
  # Center plot title
  plot.title = element_text(hjust = 0.5)
)
if (colorImp) emp.color <- color.secondary else emp.color <- "black"
ggplot() + stat_ecdf(data=df, aes(x=severities), geom = "step", color=emp.color) +
  labs(x = TeX('$s$'), y = TeX('$F_{S_n}(s)$')) + asinth.axis + xAxis + yAxis +
  ggtitle("distribución empírica de la severidad total") + theme + cdf.ylim
```

Mediante el Teorema de Glivenko-Cantelli se utiliza la aproximación normal para obtener intervalos de confianza a un nivel de $(1-\alpha)100\%$ para distribución de la severidad total $F_{S}(s)$. Entonces, para cualquier nivel de confianza, estos intervalos están dados por

$$\left(F_{s_n}(s)-\sqrt\frac{\ln\frac{2}{\alpha}}{2n}, F_{s_n}(s)+\sqrt\frac{\ln\frac{2}{\alpha}}{2n}\right)$$

Si se construyen los intervalos de confianza al $95\%$ para $F_S(s)$ utilizando la distribución empírica se tiene lo siguiente:

```{r, echo=FALSE, warning=FALSE}
Fn <- c(1:10000)
ordenados <- sort(severities)
alfa <- 0.05
limsup <- c(1:10000)
liminf <- c(1:10000)
for (i in 1:10000) {
  cuenta<-0
  for (j in 1:10000) {
    if (ordenados[j]<=ordenados[i]){
      cuenta=cuenta+1
    }
  }
  Fn[i] <- cuenta/10000
  limsup[i] <- Fn[i] + sqrt(log(2/alfa)/(2*10000))
  if (limsup[i] > 1) limsup[i] <- 1
  liminf[i] <- Fn[i] - sqrt(log(2/alfa)/(2*10000))
}
```

```{r, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
types <- c("1","2","3","4")
p <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
theme <- theme(
  # Leyend position
  legend.position = c(1, 0),
  legend.justification = c("right", "bottom"),
  legend.box.just = "right",
  # Legend boxes
  legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
  legend.key.width = unit(1.5, "lines"),
  legend.background = element_rect(fill="transparent"),
  # Center plot title
  plot.title = element_text(hjust = 0.5),
  legend.text=element_text(size=8)
)
xAxis <- geom_hline(yintercept = 0, size = .1)
yAxis <-geom_vline(xintercept = 0, size = .1)
labels <- labs(x = TeX('$s$'), y = TeX('$F_{S_n}(s)$'))
title <- ggtitle("Intervalos de confianza al 95% para \n la distribución de la severidad total")

fun2 <- geom_step(data=data.frame(ordenados,limsup, type="1"), aes(x=ordenados, y=limsup, colour=type, linetype=type, size=type)) 
fun1 <- geom_step(data=data.frame(ordenados,Fn, type="3"), aes(x=ordenados, y=Fn, colour=type, linetype=type, size=type)) 
fun3 <- geom_step(data=data.frame(ordenados,liminf, type="4"), aes(x=ordenados, y=liminf, colour=type, linetype=type, size=type)) 
cdf.layer <- geom_path(
  data=data.frame(type="2"),
  stat="function",
  fun=getValues.cdf.comp.pois,
  args=list(lambda_po=lambda_po, lambda_exp=lambda_exp),
  aes(colour=type, linetype=type, size=type)
)
an <- labs(TeX('Cota superior'), TeX('teórica'),TeX('empírica'), TeX('Cota inferior'))

if (colorImp) {
  colors <- c("#800080", "#ffa500", "#add8e6","purple")
  sizes <- c(.4,2,1,.4)
  linetypes <- c(1,1,1,1)
} else {
  colors <- c("black","black","black","black")
  sizes <- c(1,1,1,1)
  linetypes <-c(1,2,3,4)
}

p+theme+xAxis+yAxis+labels+title+cdf.layer+fun1+fun2+fun3+cdf.xlim+cdf.ylim+ asinth.axis +
scale_colour_manual(
  #name="Name",
  name = NULL,
  values = colors,
  labels = an,
  breaks = types
) +
scale_linetype_manual(
  #name="Name",
  name = NULL,
  values = linetypes,
  labels = an,
  breaks = types
) +
scale_size_manual(
  #name="Name",
  name = NULL,
  values = sizes,
  labels = an,
  breaks = types
)

```

\subsection{Comparación de los valores reales con los estimados}

Se vio que la severidad total, que sigue una distribución de Poisson Compuesta, está dada por

$$  S=\sum_{i=1}^{N}x_i.$$
Entonces, mediante la ley de esperanzas iteradas, el valor esperado de la severidad total está dado por

```{r caracteristicas de la severidad total, echo=FALSE}
mean.total.severity <- lambda_po/lambda_exp
var.total.severity <- 2*((1/lambda_exp)^2)*lambda_po
sd.total.severity <- sqrt(var.total.severity)
skewness.total.severity <- lambda_po*(6/(lambda_exp^3))
pearson.skewness.coef.total.severity <- (1/sqrt(lambda_po))*((6/(lambda_exp^3))/( 2/(lambda_exp^2))^(3/2))
kurtosis.total.severity <- ((lambda_po^4+12*lambda_po^3+36*lambda_po^2+24*lambda_po)/(lambda_exp^4))/((2*(lambda_po/(lambda_exp^2)))^2)
```

$$\begin{aligned}
E(S) &= E\left(\sum_{i=1}^{N}x_i\right)=E(E(S|N=n))=E\left(E\left(\sum_{i=1}^{n}x_i\bigg\rvert N=n\right)\right) \\
&= E(NE(x_1))=E\left(N\left(\frac{1}{\lambda_{exp}}\right)\right) \\
&=\frac{E(N)}{\lambda_{exp}}=\frac{\lambda_{Po}}{\lambda_{exp}}=\frac{`r lambda_po`}{`r lambda_exp`}=`r mean.total.severity`
\end{aligned}$$

y su varianza está dada por

$$\begin{aligned}
Var(S) &= Var(E(S|N)) + E(Var(S|N)) = Var(NE(x_1))+E(NVar(x_1)) \\
&= \frac{1}{\lambda_{exp}^2}Var(N) + \frac{1}{\lambda_{exp}^2}E(N) = \frac{\lambda_{Po}}{\lambda_{exp}^2} + \frac{\lambda_{Po}}{\lambda_{exp}^2} \\
&= 2\left(\frac{\lambda_{Po}}{\lambda_{exp}^2}\right)=2\left(\frac{`r lambda_po`}{{`r lambda_exp`}^2}\right)=`r var.total.severity`,
\end{aligned}$$

de manera que su desviacion estandar es la siguinete:

$$\sqrt{Var(S)}=\sqrt{`r sd.total.severity`}=`r sd.total.severity`.$$

La función generadora de momentos de $S$ está dada por

$$M_{S}(t)=E\left(e^{N\ln\left(e^{x_{1}t}\right)}\right)=M_{N}(\ln\left(x_{1}t\right))=e^{\lambda_{Po}\left(\frac{\lambda_{exp}}{\lambda_{exp}+t}-1\right)} $$

se sigue que el sesgo de la distribución es positivo y está dado por

$$\begin{aligned}
E\left((S-E(S))^3\right) &= \lambda_{Po}E(x_1^3)=\lambda_{Po}M^{(3)}_{x_1}(0)=\lambda_{Po}\left(\frac{6}{\lambda_{exp}^3}\right) \\
&= `r lambda_po`\left(\frac{6}{{`r lambda_exp`}^3}\right)=`r skewness.total.severity`,
\end{aligned}$$

de manera que el coeficiente de sesgo de Pearson esta dado por

$$\begin{aligned}
\gamma_{S} &= \frac{1}{\sqrt{\lambda_{Po}}}\frac{E\left(x_1^3\right)}{E\left(x_1^2\right)^{\frac{3}{2}}}=\frac{1}{\sqrt{\lambda_{Po}}}\frac{M^{(3)}_{x_1}(0)}{ E(x_1^2)^{\frac{3}{2}}}=\frac{1}{\sqrt{\lambda_{Po}}}\frac{\frac{6}{\lambda_{exp}^3}}{\left(\frac{2!}{\lambda_{exp}^2}\right)^{\frac{3}{2}}} \\
&= \frac{1}{\sqrt{`r lambda_exp`}}\frac{\frac{6}{{`r lambda_po`}^3}}{\left(\frac{2!}{{`r lambda_exp`}^2}\right)^{\frac{3}{2}}}=`r pearson.skewness.coef.total.severity`.
\end{aligned}$$

El coeficiente de Curtosis es

$$\begin{aligned}
\frac{E(S^4)}{Var(S)^2} &= \frac{M_{s}^{(4)}(0)}{\left(2\left(\frac{\lambda_{Po}}{\lambda_{exp}^2}\right)\right)^2}=\frac{\frac{\lambda_{Po}^4+12\lambda_{Po}^3+36\lambda_{Po}^2+24\lambda_{Po}}{\lambda_{exp}^4}}{\left(2\left(\frac{\lambda_{Po}}{\lambda_{exp}^2}\right)\right)^2} \\
&= \frac{\frac{{`r lambda_po`}^4+12(`r lambda_po`)^3+36(`r lambda_po`)^2+24(`r lambda_po`)}{(`r lambda_exp`)^4}}{\left(2\left(\frac{`r lambda_po`}{(`r lambda_exp`)^2}\right)\right)^2}=`r kurtosis.total.severity`.
\end{aligned}$$

Con los obtenidos por simulación, se tienen las siguientes estimaciones para la severidad total

```{r estimadores bootstrap de la severidad total, echo=FALSE}
# Media
theta1 <- function(x){mean(x)}
results <- bootstrap::bootstrap(severities,10000,theta1) 
bootstrap.mean.total.severity=mean(results[["thetastar"]])

# Varianza
theta2 <- function(x){mean(x^2)-(mean(x)^2)}
results2 <- bootstrap::bootstrap(severities,10000,theta2) 
bootstrap.variance.total.severity=mean(results2[["thetastar"]])

# Sesgo
theta3 <- function(x){mean((x-mean(x))^3)}
results3 <- bootstrap::bootstrap(severities,10000,theta3) 
bootstrap.skew.total.severity=mean(results3[["thetastar"]])

# Kurtosis
theta4 <- function(x){mean(x^4)/(var(x)^2)}
results4 <- bootstrap::bootstrap(severities, 10000, theta4)
bootstrap.kurtosis.total.severity=mean(results4[["thetastar"]])
```

```{r estimaciones jack-knife de la severidad total, echo=FALSE}
# Media
JKMean <- NULL
for(i in 1:10000) {
  JKMean[i]= (sum(severities)-severities[i])/(10000-1)
}
jackknife.mean.total.severity=mean(JKMean)

# Varianza
JKVar <- NULL
for(i in 1:10000) {
  JKVar[i] = (var(severities[-i]))
}
jackknife.variance.total.severity=mean(JKVar)

# Sesgo
JKSkew <- NULL
for(i in 1:10000) {
  JKSkew[i] = mean((severities[-i]-mean(severities))^3)
}
jackknife.skew.total.severity=mean(JKSkew)

# Kurtosis
JKKurtosis <- NULL
for(i in 1:10000) {
  JKKurtosis[i] = kurtosis(severities[-i])
}
jackknife.kurtosis.total.severity=mean(JKKurtosis)
```

```{r comparando con valores estimados, echo=FALSE}
options(scipen=999)
value <- c("Media","Varianza","Sesgo","Curtosis")
formula <- c("$\\bar{s}=\\frac{1}{n}\\sum_{i=0}^{n}s_i$","$\\frac{1}{n-1}\\sum_{i=0}^{n}\\left(s_i-\\bar{s}\\right)$","${\\frac{{\\tfrac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\overline{x}})^{3}}{\\left({\\tfrac {1}{n-1}}\\sum _{i=1}^{n}(x_{i}-{\\overline{x}})^{2}\\right)^{3/2}}}$","${\\frac{{\\tfrac{1}{n}}\\sum _{i=1}^{n}(s_{i}-{\\overline {s}})^{4}}{\\left({\\tfrac{1}{n}}\\sum _{i=1}^{n}(s_{i}-{\\overline {s}})^{2}\\right)^{2}}}-3$")
real.values <- c(mean.total.severity,var.total.severity,pearson.skewness.coef.total.severity,kurtosis.total.severity)
estimate.values <- c(mean(severities),var(severities),skewness(severities),kurtosis(severities))
bootstrap.values <- c(bootstrap.mean.total.severity, bootstrap.variance.total.severity, bootstrap.skew.total.severity, bootstrap.kurtosis.total.severity)
jackknife.values <- c(jackknife.mean.total.severity, jackknife.variance.total.severity, jackknife.skew.total.severity, jackknife.kurtosis.total.severity)
df.values <- data.frame(value, real.values, estimate.values, bootstrap.values, jackknife.values)
names(df.values) <- c("Valor","Real","Estimado","Bootstrap","Jackknife")
knitr::kable(df.values)
```

\subsection{Aproximación de la distribución de la severidad total}

La función de distribución Poisson Compuesta puede ser aproximada mediante una distribución Gamma trasladada con los siguientes parámetros: 

$$\alpha=\frac{4\lambda_{Po}E\left(x_1^{2}\right)^3}{E\left(x_1^3\right)^2}=\frac{4\lambda_{Po}\left(\frac{2}{\lambda_{exp}^2}\right)^3}{\left(\frac{6}{\lambda_{exp}^3}\right)^2}=\frac{4}{45}, \quad \beta=\frac{2E\left(x_1^2\right)}{E\left(x_1^3\right)}=\frac{2\left(\frac{2}{\lambda_{exp}^2}\right)}{\left(\frac{6}{\lambda_{exp}^3}\right)}=\frac{1}{15000}$$

y un desplazamiento

$$x_0=\lambda_{Po}E\left(x_1\right)-\frac{2\lambda_{Po}E(x_1^2)^2}{E(x_1^3)}=\lambda_{Po}\left(\frac{1}{\lambda_{exp}}\right)-\frac{2\lambda_{Po}\left(\frac{2}{\lambda_{exp}^2}\right)^2}{\left(\frac{6}{\lambda_{exp}^3}\right)}=-\frac{1000}{3}$$

Observe que la aproximación es buena para valores distantes de la media de la distribución Poisson Compuesta: 
```{r densidad vs aproximacion gamma, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
# PDF Gamma Trasladada
getValues.pdf.gamma.tras <- function(x, desp=1, alpha=4, beta=2) {
  dgamma(x-desp, shape = alpha, rate=beta)
}

# Stablish theme
comp.theme <- theme(
  #panel.grid.major = element_blank(), 
  #panel.grid.minor = element_blank(),
  #panel.background = element_blank(),
  
  # Legend position
  legend.position = c(1, 1),
  legend.justification = c("right", "top"),
  legend.box.just = "right",
  
  # Legend boxes
  legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
  legend.key.width = unit(1.5, "lines"),
  legend.background = element_rect(fill="transparent"),
  # Center plot title
  plot.title = element_text(hjust = 0.5),
  legend.text=element_text(size=8)
)

# Title
comp.title <- ggtitle("Densidad de la severidad total \n vs. \n aproximacion Gamma")

# Number of different function to be ploted
types <- c("1","2")
# Create plot
comp.p <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
# Create layers
pdf.comp.pois.layer.path <- geom_path(
  data=data.frame(type="1"),
  stat="function",
  fun = getValues.pdf.comp.pois,
  args=list(lambda_po=lambda_po, lambda_exp=lambda_exp),
  aes(colour=type, linetype=type, size=type)
)
pdf.gam.layer <- stat_function(
  data=data.frame(type="2"),
  fun = getValues.pdf.gamma.tras,
  args = list(desp=-1000/3, alpha=4/45, beta=1/15000),
  aes(colour=type, linetype=type, size=type)
)
# Colors and line types
if (colorImp) {
  aprox.color <- color.primary
  colors <- c(pdf.comp.pois.color,aprox.color)
  sizes <- c(.5,.5)
  linetypes <-c(1,1)
} else {
  colors <- c("black", "black")
  sizes <- c(.5,.5)
  linetypes <-c(1,5)
}
# Legends
comp.an <- labs(TeX('Severidad total'), TeX('aproximacion Gamma'))

# Plot
comp.p + comp.theme + xAxis + yAxis + pdf.comp.pois.labels + comp.title + pdf.xlim + pdf.ylim + pdf.comp.pois.layer.path + pdf.gam.layer +
  scale_colour_manual(
    #name="Densidades", 
    name=NULL, 
    values=colors,
    labels = comp.an,
    breaks = types
  ) + 
  scale_linetype_manual(
    #name="Densidades", 
    name=NULL,
    values = linetypes,
    labels = comp.an,
    breaks = types
  ) +
  scale_size_manual(
    #name="Densidades", 
    name=NULL,
    values = sizes,
    labels = comp.an,
    breaks = types
  )

```

Para valores cercanos a la media es mas conveniente aproximar la distribucion Poisson Compuesta con una distribucion Normal con los siguientes parametros: 

$$\mu=E(N)E(X_1)=\frac{\lambda_{Po}}{\lambda_{exp}}=(0.1)(10000)=1000, $$

$$\begin{aligned}
\sigma^2 &= Var(E(S|N)) + E(Var(S|N)) = Var(NE(x_1))+E(NVar(x_1)) \\
&= \frac{1}{\lambda_{exp}^2}Var(N) + \frac{1}{\lambda_{exp}^2}E(N) = \frac{\lambda_{Po}}{\lambda_{exp}^2} + \frac{\lambda_{Po}}{\lambda_{exp}^2} \\
&= 2\left(\frac{\lambda_{Po}}{\lambda_{exp}^2}\right)=2\left(\frac{`r lambda_po`}{{`r lambda_exp`}^2}\right)=`r var.total.severity`,
\end{aligned}$$

Sin embargo, esto solo funciona para valores grandes de $\lambda_{Po}$. En el siguinete gráfico se muestra que la aproximación Normal para valores alrededor de la media es bastante mala, pues como $\lambda_{Po}$ es pequeña la media de la suma de variables aleatorias no corverge a la media de la aproximación Normal:

```{r densidad vs aproximacion normal, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
# PDF Compound Poisson Distribution 
dpoiscomp <- function(x, lambda_po=4,lambda_exp=2) {
  # Dont take into account terms of the sum that are smaller than the following tolerance 
  tolerance <- 0.0000000001
  prob <- 0
  # The rest of terms
  n <- 1
  repeat {
    next_term <- exp(-lambda_po+n*log(lambda_po)-lfactorial(n)) * dgamma(x, shape=n, rate=lambda_exp)
    if (next_term < tolerance) {
      break
    }
    prob <- prob + next_term
    n <- n + 1
  }
  prob
}

# Get values to be ploted
getValues.pdf.pois.comp <- function(x, lambda_po=4, lambda_exp=2) sapply(x, FUN=dpoiscomp, lambda_po=lambda_po,lambda_exp=lambda_exp)

# PDF Gamma Trasladada
getValues.pdf.gamma.tras <- function(x, desp=1, alpha=4, beta=2) {
  dgamma(x-desp, shape = alpha, rate=beta)
}

# Create a plot
p <- ggplot()

# Stablish background and leyend position
theme <- theme(
  # Background
  #panel.grid.major = element_blank(), 
  #panel.grid.minor = element_blank(),
  #panel.background = element_blank(),
  
  # Legend position
  legend.position = c(1, 1),
  legend.justification = c("right", "top"),
  legend.box.just = "right",
  
  # Legend boxes
  legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
  legend.key.width = unit(1.5, "lines"),
  legend.background = element_rect(fill="transparent"),
  # Center plot title
  plot.title = element_text(hjust = 0.5),
  legend.text=element_text(size=8)
)
# Plot axes to make it look nicer
xAxis <- geom_hline(yintercept = 0, size = .1)
yAxis <-geom_vline(xintercept = 0, size = .1)

# Plot PDFs
# Labels
labels <- labs(x = TeX('$s$'), y = TeX('$f_S(s)$'))
# Title
title <- ggtitle("Densidad de la severidad total \n vs. \n aproximación Normal")
# Empty plot
# Number of different function to be ploted
types <- c("1","2")
# Create plot
p1 <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
# Create layers
pdf1 <- geom_path(
  data=data.frame(type="1"),
  stat="function",
  fun = getValues.pdf.pois.comp,
  args=list(lambda_po=0.1, lambda_exp=1/10000),
  aes(colour=type, linetype=type, size=type)
)
pdf2 <- stat_function(
  data=data.frame(type="2"),
  fun = dnorm,
  args = list(mean=1000, sd=sqrt((0.1)*(10000^2))),
  aes(colour=type, linetype=type, size=type)
)
# Colors and line types
if (colorImp) {
  aprox.color <- color.primary
  colors <- c(pdf.comp.pois.color,aprox.color)
  sizes <- c(.5,.5)
  linetypes <-c(1,1)
} else {
  colors <- c("black", "black")
  sizes <- c(.5,.5)
  linetypes <-c(1,5)
}
# Legends
an <- labs(TeX('Severidad total'), TeX('aproximación Normal'))

# Plot
p1 + theme + xAxis + yAxis + labels + title + pdf.xlim + pdf.ylim + pdf1 + pdf2 +
  scale_colour_manual(
    #name="Densidades", 
    name=NULL, 
    values=colors,
    labels = an,
    breaks = types
  ) + 
  scale_linetype_manual(
    #name="Densidades", 
    name=NULL,
    values = linetypes,
    labels = an,
    breaks = types
  ) +
  scale_size_manual(
    #name="Densidades", 
    name=NULL,
    values = sizes,
    labels = an,
    breaks = types
  )
```

En este caso, la media de la distribución de la frecuencia de los siniestros en un año es un valor muy pequeño dado por $\lambda_{Po}=`r lambda_po`$, entonces, lo más recomendable para estimar percentiles alrededor de la media, es aproximar la función de distribución mediante una distribución lognormal:

```{r densidad vs aproximacion lognormal, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
get.dlnorm <- function(x, corr=0, meanlog=0, sdlog=1) {
  dlnorm(x-corr, meanlog = meanlog, sdlog = sdlog)
}

# Plot PDFs
# Labels
labels.comp <- labs(x = TeX('$s$'), y = TeX('$f_S(s)$'))
# Title
title.comp <- ggtitle("Densidad de la severidad total \n vs. \n aproximación Lognormal")
# Empty plot
# Number of different function to be ploted
types <- c("1","2")
# Create plot
p1 <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
# Create layers
pdf1.pois.comp <- geom_path(
  data=data.frame(type="1"),
  stat="function",
  fun = getValues.pdf.pois.comp,
  args=list(lambda_po=0.1, lambda_exp=1/10000),
  aes(colour=type, linetype=type, size=type)
)
pdf2.log.norm <- stat_function(
  data=data.frame(type="2"),
  fun = get.dlnorm,
  args = list(corr=-5000,meanlog = 4, sdlog = 5),
  aes(colour=type, linetype=type, size=type)
)
# Legends
an <- labs(TeX('Severidad total'), TeX('aproximación Lognormal'))
# Plot
p1 + theme + xAxis + yAxis + labels.comp + title.comp + 
  xlim(0,80000) + 
  ylim(0,.00001) + 
  pdf1.pois.comp + pdf2.log.norm +
  scale_colour_manual(
    #name="Densidades", 
    name=NULL, 
    values=colors,
    labels = an,
    breaks = types
  ) + 
  scale_linetype_manual(
    #name="Densidades", 
    name=NULL,
    values = linetypes,
    labels = an,
    breaks = types
  ) +
  scale_size_manual(
    #name="Densidades", 
    name=NULL,
    values = sizes,
    labels = an,
    breaks = types
  )
```

Esta proximación Lognormal tiene un corrimiento igual $5000$, una media de $4$ y una desviación estándar de $5$.

\section{Estimación de la frecuencia y severidad}
  
\subsection{Simulación de los pares $\left(T_i,x_i\right)$}

Como la frecuencia de los eventos en un año se distribuye Poisson con media $\lambda_{Po}=`r lambda_po`$, entonces las diferencias de tiempo entre las ocurrencias de los eventos siguen una distribución exponencial con media $1/\lambda=\lambda_{Po}=`r lambda_po`$, es decir, los tiempos inter-arribos de reclamaciones siguen una distribución exponencial con media $1/\lambda=\lambda_{Po}=`r lambda_po`$. Simulando los tiempos inter-arribos exponenciales, se puden simular las fechas de ocurrencia de los siniestros a partir de hoy. A continuación se muestran las primeras $5$ y las últimas $5$ realizaciones de las fechas de los siniestros y los montos correspondientes (reclamaciones) distribuidos exponencialmente con media $1/\lambda_{exp}=`r siniestro.promedio`$:

```{r simulated pairs, echo=FALSE, warning=FALSE, message=FALSE}
start  <- 2018
# Tiempo interarribos
per <- rexp(10000, rate = lambda_po)
# Obtener las fechas de los siniestros
per[1] <- per[1]+start
# Fechas de los siniestros
PerTot <- cumsum(per)
# Montos de los siniestros
montos <- rexp(10000 ,rate=lambda_exp)

convert.seconds <- function(x) {
  a <- x-2018
  b <- a*31556952
  c <- as.POSIXct(b, origin = "2018-01-01", tz = "UTC")
  as.character(c)
}
dates <- sapply(PerTot, convert.seconds)

archivo <- data.frame(dates, montos)
names(archivo) <- c("Fechas", "Montos")
primeras <- head(archivo, 5)
kable(primeras, row.names = NA)
```

$$\vdots  $$
```{r cola, echo=FALSE, warning=FALSE}
ultimas <- tail(archivo, 5)
chacada <- data.frame(ultimas$Fechas, ultimas$Montos)
names(chacada) <- c("Fechas", "Montos")
kable(chacada)
```

\pagebreak

\subsection{Serie de tiempo} 

```{r total de informacion, echo=FALSE, warning=FALSE}
conver <- function(x) x-2018
tiempoEnAnios <- sapply(PerTot, conver)
```

A continuación se muestra la serie de tiempo generada con la informacion de $`r floor(max(tiempoEnAnios))`$ años:

```{r, serie de tiempo, echo=FALSE, fig.height = 12, fig.width = 10}
# Ploting time series
# https://www.neonscience.org/dc-time-series-plot-ggplot-r
theme <- theme(
  # Leyend position
  legend.position = c(1, 0),
  legend.justification = c("right", "bottom"),
  legend.box.just = "right",
  # Legend boxes
  legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
  legend.key.width = unit(1.5, "lines"),
  legend.background = element_rect(fill="transparent"),
  # Center plot title
  plot.title = element_text(hjust = 0.5),
  legend.text=element_text(size=8),
  axis.text.x=element_text(size=14)
)
fechonsias <- archivo[[1]]
funciononsia <- function(x) {
  value <- NA
  if (match(x,fechonsias) %% 500 !=0) {
    value <- NA
  } else {
    fechaSeparada <- strsplit(as.character(x), " ")
    value <- fechaSeparada[[1]][1]
  }
  value
}
fechunias <- sapply(fechonsias, funciononsia)
montunios <- archivo[[2]]
dfonsio <- data.frame(fechunias, montunios)
names(dfonsio) <- c("fechonson", "montonson")

ggplot(data = dfonsio, aes(x = c(1:10000), y = montonson, group = 1)) + 
  geom_bar(stat="identity", color = "#00AFBB", size = .1, alpha=.1, na.rm = TRUE) +
  geom_point(size=.005, color="blue", alpha=0.5) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size=5)) + 
  scale_x_discrete(limits=dfonsio[[1]], na.translate = FALSE) + xlab("Fecha") + ylab("Monto") + 
  geom_hline(yintercept=10000, size=1, color="cyan") + coord_flip() + scale_y_reverse() + theme
```

La linea azul claro indica la media de la distribución teorica a partir de la cual fueron simulados los datos.

\subsection{Distribución empírica de la severidad}

Con los valores simulados de los montos, se puede construir la función de distribución de probabilidad empírica de la severidad:

```{r calculo empirica severidad, echo=FALSE, warning=FALSE}
Fn.sev <- c(1:10000)
empirica<-ecdf(montos)
montosOrdenados <- sort(montos)
alfa.sev <- 0.05
limsup.sev <- c(1:10000)
liminf.sev <- c(1:10000)
for (i in 1:10000) {
  cuenta<-0
  for (j in 1:10000) {
    if (montosOrdenados[j]<=montosOrdenados[i]){
      cuenta=cuenta+1
    }
  }
  Fn.sev[i] <- cuenta/10000
  limsup.sev[i] <- Fn.sev[i] + sqrt(log(2/alfa.sev)/(2*10000))
  if (limsup.sev[i] > 1) limsup.sev[i] <- 1
  liminf.sev[i] <- Fn.sev[i] - sqrt(log(2/alfa)/(2*10000))
}
```

```{r, distribucion empirica de la severidad, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
types <- c("1","2","3","4")
p <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
theme <- theme(
  # Leyend position
  legend.position = c(1, 0),
  legend.justification = c("right", "bottom"),
  legend.box.just = "right",
  # Legend boxes
  legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
  legend.key.width = unit(1.5, "lines"),
  legend.background = element_rect(fill="transparent"),
  # Center plot title
  plot.title = element_text(hjust = 0.5),
  legend.text=element_text(size=8)
)
xAxis <- geom_hline(yintercept = 0, size = .1)
yAxis <-geom_vline(xintercept = 0, size = .1)
labels <- labs(x = TeX('$m$ (montos)'), y = TeX('$F_{M_n}(m)$'))
title <- ggtitle("Intervalos de confianza al 95% para \n la distribución de la severidad")

montosOrdenados <- sort(montos)
fun2 <- geom_step(data=data.frame(montosOrdenados,limsup, type="1"), aes(x=montosOrdenados, y=limsup.sev, colour=type, linetype=type, size=type)) 
fun1 <- geom_step(data=data.frame(montosOrdenados,Fn, type="3"), aes(x=montosOrdenados, y=Fn.sev, colour=type, linetype=type, size=type)) 
fun3 <- geom_step(data=data.frame(montosOrdenados,liminf, type="4"), aes(x=montosOrdenados, y=liminf.sev, colour=type, linetype=type, size=type)) 
cdf.layer <- geom_path(
  data=data.frame(type="2"),
  stat="function",
  fun=pexp,
  args=list(rate=lambda_exp),
  aes(colour=type, linetype=type, size=type)
)
an <- labs(TeX('Cota superior'), TeX('teórica'),TeX('empírica'), TeX('Cota inferior'))

if (colorImp) {
  colors <- c("#800080", "#ffa500", "#add8e6","purple")
  sizes <- c(.4,2,1,.4)
  linetypes <- c(1,1,1,1)
} else {
  colors <- c("black","black","black","black")
  sizes <- c(1,1,1,1)
  linetypes <-c(1,2,3,4)
}

p+theme+xAxis+yAxis+labels+title+cdf.layer+fun1+fun2+fun3+cdf.xlim+ylim(0,1)+ asinth.axis +
scale_colour_manual(
  #name="Name",
  name = NULL,
  values = colors,
  labels = an,
  breaks = types
) +
scale_linetype_manual(
  #name="Name",
  name = NULL,
  values = linetypes,
  labels = an,
  breaks = types
) +
scale_size_manual(
  #name="Name",
  name = NULL,
  values = sizes,
  labels = an,
  breaks = types
)
```

Dado el numero de observaciones (simulaciones) $n=10000$, se tiene que la función de distribución empírica de la severidad se aproxima bastante bien a la función de distribución teórica y la longitud de los intervalos de confianza no es muy grande. Note que las cotas superiores e inferiores "abrazan estrechamente" a la función de distribución teórica. 

\subsection{Histograma de la severidad}

Agrupando los montos de los siniestros en intervalos de longitud $5000$, se obtiene el siguinete histograma:
```{r histograma severidad, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
if (colorImp) {
  hist.color <- color.primary
  hist.fill <- color.primary
  hist.alpha <- "0.2"
} else {
  hist.color <- "black"
  hist.fill <- "white"
  hist.alpha <- "1"
}

montos.df <- data.frame(montos)
breaks <- seq(1, 80000, by=5000)
hist.p <- ggplot(data=montos.df, aes(x=montos)) +
  geom_histogram(breaks=breaks, color=hist.color, fill=hist.fill, alpha=hist.alpha) + 
  labs(x = TeX('Severidad$'), y = TeX('Frecuencia')) +
  ggtitle("Histograma de la severidad") + title.theme
hist.p
```

Con un escalamiento adecuado sobre las frecuencias (note el cambio en la escala en el eje $y$), se puede observar la enorme similitud entre la densidad y el histograma:

```{r  histograma con densidad sev, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}

pdf.sev.layer <- stat_function(
  aes(x=c(0,10)),
  fun=dexp,
  args=list(rate=lambda_exp),
  color=color.secondary
)

hist.p2.sev <- ggplot() + xAxis + yAxis + title.theme + pdf.xlim + ylim(0,.0001) + geom_histogram(data=montos.df, aes(x=montos, y=.00000002*..count..), breaks=breaks, color=hist.color, fill=hist.fill, alpha=hist.alpha) + pdf.sev.layer +
  ggtitle("función de densidad de probabilidad \n de la severidad total") +
  labs(x = TeX('$s$'), y = TeX('$f_S(s)$'))
hist.p2.sev
```

\subsection{Características de la distribución empírica}

Sea $M$ la variable aleatoria que indica el monto de un siniestro, entonces el valor esperado de $M$ limitado a $l$ está dada por

$$E\left(\min{M, l}\right)=\int_{0}^{l}S_{x_1}(t)\,\mathrm{d}t=\int_{0}^{l} e^{-\lambda_{exp}t}\,\mathrm{d}t=\frac{1-e^{-\lambda_{exp}l}}{\lambda_{exp}}$$

A continuación se presenta a la \textbf{media limitada} de los montos como función del límite:

```{r media limitada, echo=FALSE, warning=FALSE, message=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
sortedSeverities <- sort(unique(montos))
 
# Medias limitadas usando datos empiricos
n=10000
mediasLimitadas <- NULL
for (i in 1:length(sortedSeverities)) {
  u <- sortedSeverities[i]
  mediasLimitadas[i] <- (sum(montos>u)*u + sum(montos[montos<=u]))/n
}

# Media limitada analiticamente como funcion del limite
limitedMean <- function(x) (1-exp(-lambda_exp*x))/(lambda_exp)

types <- c("1","2")
p.m <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
theme <- theme(
   # Leyend position
   legend.position = c(1, 0),
   legend.justification = c("right", "bottom"),
   legend.box.just = "right",
   # Legend boxes
   legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
   legend.key.width = unit(1.5, "lines"),
   legend.background = element_rect(fill="transparent"),
   # Center plot title
   plot.title = element_text(hjust = 0.5),
   legend.text=element_text(size=8)
 )
 xAxis <- geom_hline(yintercept = 0, size = .1)
 yAxis <-geom_vline(xintercept = 0, size = .1)
 labels <- labs(x = TeX('$s$'), y = TeX('$F_{S_n}(s)$'))
 title <- ggtitle("Función media limiatada")
 
 points <- geom_point(
   data=data.frame(sortedSeverities, mediasLimitadas, type="1"),
   aes(x=sortedSeverities, y=mediasLimitadas, colour=type, linetype=type, size=type),
   shape=0
 )
 
 curve <- geom_path(
   data=data.frame(type="2"),
   stat="function",
   fun=limitedMean,
   aes(colour=type, linetype=type, size=type)
 )
 
 an <- labs(TeX('Valores simulados'), TeX('Media limitada'))
 
 if (colorImp) {
   colors <- c(color.primary,color.secondary)
   sizes <- c(.3,1)
   linetypes <- c(1,1)
 } else {
   colors <- c("black","black")
   sizes <- c(1,1)
   linetypes <-c(1,2)
 }
 
 p.m+theme+xAxis+yAxis+labs(x="Limite", y="Media limitada")+title+curve+points+xlim(0,80000)+ylim(0,10000)+ asinth.axis +
 scale_colour_manual(
   #name="Name",
   name = NULL,
   values = colors,
   labels = an,
   breaks = types
 ) +
 scale_linetype_manual(
   #name="Name",
   name = NULL,
   values = linetypes,
   labels = an,
   breaks = types
 ) +
 scale_size_manual(
   #name="Name",
   name = NULL,
   values = sizes,
   labels = an,
   breaks = types
 )
```

Como los montos son distribuidos exponencialmente con media $E(x_1)=1/\lambda_{exp}$, no se observan indicios de una distribución de cola pesada.
 
 La \textbf{media en exceso} como funcion del umbral es la siguiente:

```{r media en exceso, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
 # Medias en exceso usando datos empiricos
 mediasEnExceso <- NULL
 for(i in 1:length(sortedSeverities)){
   d <- sortedSeverities[i]
   mediasEnExceso[i] <- sum(montos[montos>d]-d)/sum(montos>d)
 }
 
# Media limitada analiticamente como funcion del limite
 excesMean <- function(x, param=1) param
 
 types <- c("1","2")
 p.m <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
 theme <- theme(
   # Leyend position
   legend.position = c(1, 0),
   legend.justification = c("right", "bottom"),
   legend.box.just = "right",
   # Legend boxes
   legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
   legend.key.width = unit(1.5, "lines"),
   legend.background = element_rect(fill="transparent"),
   # Center plot title
   plot.title = element_text(hjust = 0.5),
   legend.text=element_text(size=8)
 )
 xAxis <- geom_hline(yintercept = 0, size = .1)
 yAxis <-geom_vline(xintercept = 0, size = .1)
 labels <- labs(x = TeX('$s$'), y = TeX('$F_{S_n}(s)$'))
 title <- ggtitle("Función media en exceso")
 
 points <- geom_point(
   data=data.frame(sortedSeverities, mediasEnExceso, type="1"),
   aes(x=sortedSeverities, y=mediasEnExceso, colour=type, linetype=type, size=type),
   shape=1
 )
 
 curve <- geom_path(
   data=data.frame(type="2"),
   stat="function",
   fun=excesMean,
   args=list(param=10000),
   aes(colour=type, linetype=type, size=type)
 )
 
 an <- labs(TeX('Valores simulados'), TeX('Media en exceso'))
 
 p.m+theme+xAxis+yAxis+labs(x="Umbral", y="Media en exceso")+title+curve+points+xlim(0,80000)+ylim(0,15000)+ asinth.axis +
 scale_colour_manual(
   #name="Name",
   name = NULL,
   values = colors,
   labels = an,
   breaks = types
 ) +
 scale_linetype_manual(
   #name="Name",
   name = NULL,
   values = linetypes,
   labels = an,
   breaks = types
 ) +
 scale_size_manual(
   #name="Name",
   name = NULL,
   values = sizes,
   labels = an,
   breaks = types
 )
```

Claramente los montos de los siniestros no son un fenomeno de colas pesadas, pues se observa que la media en exceso descrece.
 
La \text{mortalidad empirica} como función del monto es la siguiente:

```{r mortalidad empirica, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
 # Mortalidad utilizando datos empirica
 fn <- NULL
 mortalidades <- NULL
 for (i in 1:length(sortedSeverities)) {
   fn[i] <- empirica(sortedSeverities[i+1])-empirica(sortedSeverities[i])
   mortalidades[i] <- fn[i]/(1-empirica(sortedSeverities[i]))
 }
 
 # Funcion de mortalidad
 hazardRate <- function(x, param=1) 1/param
 
 types <- c("1","2")
 p.m <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
 theme <- theme(
   # Leyend position
   legend.position = c(0.05, 1),
   legend.justification = c("left", "top"),
   legend.box.just = "right",
   # Legend boxes
   legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
   legend.key.width = unit(1.5, "lines"),
   legend.background = element_rect(fill="transparent"),
   # Center plot title
   plot.title = element_text(hjust = 0.5),
   legend.text=element_text(size=8)
 )
 xAxis <- geom_hline(yintercept = 0, size = .1)
 yAxis <-geom_vline(xintercept = 0, size = .1)
 labels <- labs(x = TeX('$s$'), y = TeX('$F_{S_n}(s)$'))
 title <- ggtitle("Función mortalidad")
 
 points <- geom_point(
   data=data.frame(sortedSeverities, mortalidades, type="1"),
   aes(x=sortedSeverities, y=mortalidades, colour=type, linetype=type, size=type),
   shape=1
 )
 
 curve <- geom_path(
   data=data.frame(type="2"),
   stat="function",
   fun=hazardRate,
   args=list(param=10000),
   aes(colour=type, linetype=type, size=type)
 )
 
 an <- labs(TeX('Valores simulados'), TeX('Mortalidad = 1/10000'))
 
 p.m+theme+xAxis+yAxis+labs(x="Montos", y="Mortalidad")+title+curve+points+xlim(0,85000)+ylim(0,1)+ asinth.axis +
 scale_colour_manual(
   #name="Name",
   name = NULL,
   values = colors,
   labels = an,
   breaks = types
 ) +
 scale_linetype_manual(
   #name="Name",
   name = NULL,
   values = linetypes,
   labels = an,
   breaks = types
 ) +
 scale_size_manual(
   #name="Name",
   name = NULL,
   values = sizes,
   labels = an,
   breaks = types
 )
```

\subsection{Prueba Ji-Cuadrada de Pearson para la severidad}

Se agrupan los datos de tal forma que todas las frecuencias de los datos observados sean mayores a 5:

```{r Prueba Chi-Square, echo=FALSE, warning=FALSE}
##Datos agrupados
nj<-NULL
agrupados1<-data.frame(cj_1=seq(0,max(sortedSeverities),5000), cj=seq(5000,max(sortedSeverities)+5000,5000))
                      
for(i in 1:length(agrupados1$cj_1)){
  nj[i]<-sum(montos<agrupados1$cj[i] & agrupados1$cj_1[i] <= montos)
}
agrupados1[3]<-nj
row.names(agrupados1)<-c(1:length(agrupados1$V3))

 agrupados<-agrupados1
k=1
while(agrupados$V3[k]>=5){
  aux <- k
k=k+1
}

  aux2<-0
  for(i in 1:(length(agrupados$V3)-aux)){
    aux2<-aux2+agrupados$V3[aux+i]
  }
agrupados$V3[aux+1]<-aux2

for(i in 1:(length(agrupados$cj)-aux-1)){
   agrupados$V3[aux+1+i]<-0
}

agrupados$cj[aux+1]<- agrupados$cj[length(agrupados$cj_1)]
 agrupados<-subset(agrupados, agrupados$V3>0) 
 agrupados<-rbind(agrupados,1)
 agrupados$cj_1[length(agrupados$V3)]<-agrupados$cj[length(agrupados$V3)-1]
 agrupados$cj[length(agrupados$cj)]<-"inf"
 agrupados$V3[length(agrupados$V3)]<-0
 kable(agrupados)

 ##Estimo el parámetro para una exponencial por MV
theta<-1/mean(montos)
E<-NULL
E[1]<-10000*(pexp(5000,theta))
for(i in 2:(length(agrupados$cj_1)-1)){
  E[i]<-10000*(pexp(as.numeric(agrupados$cj[i]),theta)-pexp(as.numeric(agrupados$cj_1[i]),theta))
}
E[length(agrupados$cj_1)]=10000*(1-pexp(as.numeric(agrupados$cj_1[length(agrupados$cj_1)]),theta))
agrupados[4]<-E

Q<-NULL
for(i in 1:(length(agrupados$cj_1))) {
  Q[i]<-((as.numeric(agrupados$V3[i])-as.numeric(agrupados$V4[i]))^2)/as.numeric(agrupados$V3[i])
}
Q[length(agrupados$cj_1)]=0
agrupados[5]<-Q
sumaQ<-sum(Q)
gl<-length(agrupados$cj_1)-3
test<-qchisq(.95,gl)

Rechazar <- sumaQ > test
```

Realizando la prueba Ji-Cuadrada, se tiene que la estadística de prueba toma el siguite valor:
$$Q=`r sumaQ` < \chi_{(`r gl`),0.05}^2 = `r test`$$


Por lo tanto, la hipótesis de que la severidad sigue una distribución exponencial con media $10000$ no es rechazada.

```{r prueba, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
ggplot(data=data.frame(num=c("1","2","3"))) +
  stat_function(
    fun = dchisq,
    args = list(df = gl),
    xlim = c(test, 40),
    geom = "area",
    fill = color.primary,
    color = "transparent",
    alpha = ".5"
    ) +
stat_function(
  fun = dchisq,
  args = list(df = gl),
  color = color.primary
) +
geom_vline(
  aes(
    xintercept = sumaQ
  ),
  color = color.secondary
) +
geom_vline(
  aes(
    xintercept = test
  ),
  color="black"
) +
xlim(0,40) +
ylim(0,0.1) +
  labs(x = TeX('$x$'), y = TeX('$ f_{\\chi_{(14)}^2}(x) $')) + xAxis + yAxis + ggtitle("Prueba Ji-Cuadrada al 95%") + theme +
  annotate("text", x = 30, y = 0.01, label = "alpha==0.05", parse=TRUE) + 
  annotate("text", x = 10, y = 0.005, label = paste("Q==", sumaQ), parse=TRUE) +
  annotate("text", x = 30, y = 0.075, label = paste("{chi^{2}} ==", test), parse=TRUE)  
```

\subsection{Q-Q Plot}

Derivado del q-q plot podemos concluir que el modelo subestima las probabilidades de eventos de monto alto:

```{r q-q plot, echo=FALSE, warning=FALSE, message=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
geom_qq_line <- function(mapping = NULL,
                         data = NULL,
                         geom = "path",
                         position = "identity",
                         ...,
                         distribution = stats::qnorm,
                         dparams = list(),
                         line.p = c(.25, .75),
                         fullrange = FALSE,
                         na.rm = FALSE,
                         show.legend = NA,
                         inherit.aes = TRUE) {
  layer(
    data = data,
    mapping = mapping,
    stat = StatQqLine,
    geom = geom,
    position = position,
    show.legend = show.legend,
    inherit.aes = inherit.aes,
    params = list(
      distribution = distribution,
      dparams = dparams,
      na.rm = na.rm,
      line.p = line.p,
      fullrange = fullrange,
      ...
    )
  )
}

StatQqLine <- ggproto("StatQqLine", Stat,
 default_aes = aes(x = stat(x), y = stat(y)),
 required_aes = c("sample"),
 compute_group = function(data,
                          scales,
                          quantiles = NULL,
                          distribution = stats::qnorm,
                          dparams = list(),
                          na.rm = FALSE,
                          line.p = c(.25, .75),
                          fullrange = FALSE) {

   sample <- sort(data$sample)
   n <- length(sample)

   # Compute theoretical quantiles
   if (is.null(quantiles)) {
     quantiles <- stats::ppoints(n)
   } else {
     stopifnot(length(quantiles) == n)
   }

   theoretical <- do.call(
     distribution,
     c(list(p = quote(quantiles)), dparams)
   )

   if (length(line.p) != 2) {
     stop(
       "Cannot fit line quantiles ", line.p,
       ". Parameter line.p must have length 2.",
       call. = FALSE)
   }

   x_coords <- do.call(distribution, c(list(p = line.p), dparams))
   y_coords <- quantile(sample, line.p)
   slope <- diff(y_coords) / diff(x_coords)
   intercept <- y_coords[1L] - slope * x_coords[1L]

   if (fullrange & !is.null(scales$x$dimension)) {
     x <- scales$x$dimension()
   } else {
     x <- range(theoretical)
   }

   data.frame(x = x, y = slope * x + intercept)
 }
)

ggplot(data = data.frame(y = montosOrdenados), aes(sample = y)) + 
  geom_qq(distribution = qexp, dparams = list(rate = lambda_exp), color = color.primary) + geom_qq_line(distribution = qexp, dparams = list(rate = lambda_exp)) + xlab(TeX("Cuantiles $exp(\\lambda_{exp})$")) + ylab(TeX("Montos")) + ggtitle("Q-Q Plot") + theme
```


\subsection{Estimación para la frecuencia}

```{r estimacion frecuencia, echo=FALSE, warning=FALSE}
# Ocurrencias en cada año
ocurrencias <- seq(0, 0, length.out = floor(max(tiempoEnAnios)))
for (j in 1:length(tiempoEnAnios)) {
  tiempo <- tiempoEnAnios[j]
  anio <- floor(tiempo)
  ocurrencias[anio] <- ocurrencias[anio] + 1
}
tasa <- sum(ocurrencias)/length(ocurrencias)
desviacion <- 1.96*sqrt(tasa/10000) 
liminferior <- tasa-desviacion
limsuperior <- tasa+desviacion
```
Suponiendo que la frecuencia de los siniestros se distribuye Poisson, entonces, estimando por máxima verocimilitud se obtiene que, en promedio, ocurren 
$$\widehat{\lambda}_{MV}=\frac{1}{T}\sum_{j=0}^{T}N_i=\frac{1}{`r floor(max(tiempoEnAnios))`}\sum_{j=0}^{`r floor(max(tiempoEnAnios))`}N_i=`r tasa` $$
siniestros al año. En donde $T$ es el total de años sobre los cuales se tiene información y $N_j$ es el número de siniestros que ocurrieron en el $j$-ésimo año. Además, dados estos valores se tiene que un intervalo al $95\%$ de confianza para el parámetro de esta distribución Poisson es el siguiente:

$$\widehat{\lambda}\pm 1.96 \sqrt{\dfrac{\widehat{\lambda}}{n}}=`r tasa`\pm `r desviacion` =(`r liminferior`,`r limsuperior`).$$

\section{Estimación con censura y truncamiento}

\subsection{Efectos de un deducible y un límite}

Si se aplica un deducible de $5,000$ y un límite de $30,000$, entonces el archivo de indemnizaciones tendrá el siguiente aspecto:
```{r montos simulados, echo=FALSE, warning=FALSE}
##TRUNCAMIENTO
Indem<-montos
for (i in 1:10000) {
  if (Indem[i] > 30000) {
    Indem[i] = 30000
  } else if (Indem[i] < 5000) {
    Indem[i] = 0
  }
}
Indemnizacion<-as.data.frame(Indem)
#Indem[1:100]
Indem1<-subset(Indem,Indem !=0)

limite <- 30000
deducible <- 5000
fechasModif <- c()
montosModif <- c()
fechas <- archivo[[1]]
montos <- archivo[[2]]
for (i in 1:length(montos)) {
  if (montos[i] > limite) {
    montosModif <- c(montosModif, limite)
    fechasModif <- c(fechasModif, as.character(fechas[i]))
  } else if (montos[i] > deducible) {
    montosModif <- c(montosModif, montos[i])
    fechasModif <- c(fechasModif, as.character(fechas[i]))
  }
}
archivoModif <- data.frame(fechasModif, montosModif)
eliminados <- (1-(length(archivoModif[[1]])/length(archivo[[1]])))*100
eliminadosTeoricos <- pexp(deducible, rate = lambda_exp)*100
eliminadosTeoricos.2 <- pexp(deducible, rate = lambda_exp)
names(archivoModif) <- c("Fechas","Montos")
primerasModif <- head(archivoModif, 5)
kable(primerasModif)
```

$$\vdots$$
```{r cola archivo modif, echo=FALSE, warning=FALSE}
ultimasModif <- tail(archivoModif, 5)
chacadaModif <- data.frame(ultimasModif$Fechas, ultimasModif$Montos)
names(chacadaModif) <- c("Fechas", "Montos")
kable(chacadaModif)
```

Como resultado de imponer un deducible y un límite, se ha eliminado un  $`r eliminados`\%$ de registros en el archivo de indemnizaciones. Se aceptó la hipótesis de que los montos de los siniestros provienen de una distribución exponencial de media $`r siniestro.promedio`$, entonces se puede estimar de manera anaítica la proporción de registros eliminados mediante la probabilidad de que el monto de un siniestro se encuentre por debajo del deducible de $`r deducible`$. Esta probabilidad es la siguiente:

$$ Pr\{M\leq `r deducible`\}=1-e^{-\frac{`r deducible`}{`r siniestro.promedio`}}=`r eliminadosTeoricos.2`.$$

\pagebreak

La nueva seríe de tiempo generada por el proceso de riesgo es la siguiente:

```{r, serie de tiempo con deducible y limite, echo=FALSE, fig.height = 12, fig.width = 10}
# Ploting time series
# https://www.neonscience.org/dc-time-series-plot-ggplot-r
theme <- theme(
  # Leyend position
  legend.position = c(1, 0),
  legend.justification = c("right", "bottom"),
  legend.box.just = "right",
  # Legend boxes
  legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
  legend.key.width = unit(1.5, "lines"),
  legend.background = element_rect(fill="transparent"),
  # Center plot title
  plot.title = element_text(hjust = 0.5),
  legend.text=element_text(size=8),
  axis.text.x=element_text(size=14)
)
fechonsias <- archivoModif[[1]]
funciononsia <- function(x) {
  value <- NA
  if (match(x,fechonsias) %% 500 !=0) {
    value <- NA
  } else {
    fechaSeparada <- strsplit(as.character(x), " ")
    value <- fechaSeparada[[1]][1]
  }
  value
}
fechunias <- sapply(fechonsias, funciononsia)
montunios <- archivoModif[[2]]
dfonsio <- data.frame(fechunias, montunios)
names(dfonsio) <- c("fechonson", "montonson")

ggplot(data = dfonsio, aes(x = c(1:length(dfonsio$montonson)), y = montonson, group = 1)) + 
geom_bar(stat="identity", color = "#00AFBB", size = .1, alpha=.1, na.rm = TRUE) +
geom_point(size=.005, color="blue", alpha=0.5) +
theme(axis.text.x = element_text(angle = 90, hjust = 1, size=5)) + 
scale_x_discrete(limits=dfonsio[[1]], na.translate = FALSE) + 
xlab("Fecha") + 
ylab("Monto") + 
geom_hline(yintercept=10000, size=1, color="cyan") + 
coord_flip() +
scale_y_reverse(limits=c(85000,0)) +
theme
```

La línea azul claro indica el valor de la media en exceso con un umbral de $`r deducible`$.

\subsection{Estimación para montos con deducible y límite}

Primero vamos a definir la función con el deducible de $5,000$ y un límite de $30,000$

$$
y(x) = \begin{cases}
  0 & x\leq 5,000\\
  x-5000 & 5,000\leq x \leq 30,000\\
  25,000 & 30,000\leq x
\end{cases}
$$

$$F_{Y}(y) = \begin{cases}
 0 & y = 0\\
 \frac{F_{x}(y + 5000) - F_{x}(5000)}{1-F_{x}(5000)}& 0\leq y \leq 900\\
  1  & 900\leq y
\end{cases}$$


$$f_{Y}(y) = \begin{cases}
 \frac{f_{x}(y + 5000)}{S_{X}(5000)} & 0\leq y \leq 25,000\\
 \frac{1- F_{x}(25000)}{1-F_{x}(5000)} & y = 25,000
\end{cases}$$

Un vez teniendo bien definidas funciones de Distribución y Densidad, podemos hacer la función de Máxima Verosimilitud.

$$L(\lambda) =   \left(\frac{1- F_{x}(25000)}{1- F_{x}(5000)}\right)^a \cdot\prod_{i=1}^b \frac{f_{x}(y_{i} + 5000)}{1-F_{x(5000)}}$$

Ahora, sustituyendo los valores de la función definida, se tiene que:

$$L(\lambda) = \left(\frac{e^{-\lambda(25000)}}{e^{-\lambda(5000)}}\right)^a\cdot\prod_{i=1}^b \frac{\lambda e^{-\lambda(y_{i} + 5000)}}{e^{-\lambda(5000)}}$$

sacando Logaritmo a la función:

$$\ln(\lambda) = a(-\lambda(25000) + \lambda(5000)) + \sum_{i=1}^b \left( \ln(\lambda)-\lambda(y_{i} + 5000) + \lambda(5000)a\right)$$  

Agrupando los terminos comunes llegamos a la función a maximizar

$$l(\lambda) = a(-\lambda(2000)) +  b(\ln(\lambda)) - \sum_{i=1}^by_{i}$$


```{r estimacion de lambda exponencial, echo=FALSE, warning=FALSE}

Indem2<-subset(Indem,Indem == 25000)
Indem3<-subset(Indem1,Indem1!= 25000)
Indem25<-length(Indem2)
Indemsin25<-length(Indem3)

Flog<-function(x){
 length(Indem2)*(-x*20000)+length(Indem3)*logb(x)- x*sum(Indem3)
}

##Resolviendo con Solver en Excel, tenemos que:
lambdaMV = .0000765113632847696
mediaEst = 1/lambdaMV
mediaDatos= mean(Indem1)

##Podemos observar que la media de los datos es mayor que la media estimada ya que la segunda considera la probabilidad de los siniestros menores al deducible

##La proporción de siniestros pagados calculada analíticamente es:
EstimAnalitica<-exp(-5000*lambdaMV)*10000
```

Después de resolver el problema optimización, se obtuvo que el parámetro de la distribución exponencial estimado por máxima verosimilitud es $.000076511$ y su media $13,069.95$. Comparada con la media que se obtuvo de los datos, que fue de $13,734.35$, se puede notar que el parámetro estimado considera los efectos de censura y truncamiento adecuadamente.

\section{Conclusiones}

La simulación de la frecuencia y la serveridad, para los compañías aseguradoras es muy importante, pues con ellas pueden, mediante diversas técnicas de muestreo, simular el comportamiento de las reclamaciones para obtener la distribución del monto agregado que terminará desembolsando la compañía en determinado periodo de tiempo. 

En este caso se conoce la distribución compuesta para al serveridad total, sin embargo, una vez realizadas las pruebas de ajuste y determinadas las distribuciones para la severidad total, estás distribuciones pueden ser ajustadas bayesianamente obteniendo expresiones con las que es imposible trabajar analíticamente. Para resolver el problema, los actuarios utilizan métodos similares a los que usaron en este trabajo para realizar todos los cálculos necesarios para cobrar lo justo, reservar las cantidades adecuadas de dinero y mantener a flote la compañía asegurando un nivel deseable de utilidades para los accionistas. 

Aquí se destaca la importancia de la simulación, pues sin ella, no sería posible modelar la realidad adecuadamente provocando una mala gestión de recursos y descontento ante la incertidumbre. 

